# AffectLink: Multimodal Emotion Consistency Tracking for Telehealth

**AffectLink** is an AI-powered system that analyzes a patientâ€™s emotional state by comparing facial expressions with vocal tone. It helps identify inconsistencies, fatigue, or stress â€” providing clinicians with real-time and retrospective emotional insights during virtual sessions.

## ğŸ” What It Does
- ğŸ™ï¸ Detects speech-based emotional tone using Whisper + emotion classification.
- ğŸ¥ Detects facial emotion using real-time video feed.
- ğŸ§© Compares the two to assess emotional consistency or mismatch.
- ğŸ“ˆ Tracks mood and stress markers over time to surface potential concerns.
- ğŸ§‘â€âš•ï¸ Built for clinicians and therapists using telehealth platforms.

## ğŸš€ Features
- Multimodal emotional analysis (audio + video).
- Session-level emotional summary reports.
- Local-first privacy-preserving deployment.
- Optional integration with MLflow and Swagger.

## ğŸ› ï¸ Built With
- Python
- OpenCV, DeepFace, Whisper
- Streamlit / Flask
- MLflow (optional)
- HP AI Studio or local NVIDIA hardware

## ğŸ§ª Setup Instructions
```bash
git clone https://github.com/yourusername/emotiva.git
cd emotiva
pip install -r requirements.txt
python app/main.py
```

## ğŸ“„ Demo Video
- [Link to Demo Video Here]

## ğŸ“š Documentation
- [Architecture Diagram](docs/architecture.png)
- [Demo Scenario Script](docs/demo_script.md)

## âœ¨ Future Work
- Add sentiment summarization of conversation.
- Improve emotion classification with lightweight transformers.
- Extend to triadic (3+ participants) sessions.

## ğŸ“œ License
MIT License

---
Built with ğŸ’¬ by Kevin King for the HP AI Studio & NVIDIA Developer Challenge.